Epoch 1
-------------------------------

<ipython-input-4-89b71ddc6ddc>:69: DeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)
  box = np.int0(box)

loss: 8.178527  [   32/13903]
loss: 0.685157  [ 3232/13903]
loss: 0.623652  [ 6432/13903]
loss: 0.304542  [ 9632/13903]
loss: 0.524938  [12832/13903]
Test Error: 
 Accuracy: 57.9%, Avg loss: 0.678920 

Epoch 2
-------------------------------
loss: 0.309610  [   32/13903]
loss: 0.485015  [ 3232/13903]
loss: 0.683302  [ 6432/13903]
loss: 0.280972  [ 9632/13903]
loss: 0.301654  [12832/13903]
Test Error: 
 Accuracy: 84.5%, Avg loss: 0.340645 

Epoch 3
-------------------------------
loss: 0.254787  [   32/13903]
loss: 0.316590  [ 3232/13903]
loss: 0.406693  [ 6432/13903]
loss: 0.222791  [ 9632/13903]
loss: 0.349807  [12832/13903]
Test Error: 
 Accuracy: 86.7%, Avg loss: 0.319930 

Epoch 4
-------------------------------
loss: 0.177960  [   32/13903]
loss: 0.328773  [ 3232/13903]
loss: 0.315293  [ 6432/13903]
loss: 0.155329  [ 9632/13903]
loss: 0.232385  [12832/13903]
Test Error: 
 Accuracy: 87.1%, Avg loss: 0.333561 

Epoch 5
-------------------------------
loss: 0.252418  [   32/13903]
loss: 0.250769  [ 3232/13903]
loss: 0.221205  [ 6432/13903]
loss: 0.092774  [ 9632/13903]
loss: 0.336152  [12832/13903]
Test Error: 
 Accuracy: 85.9%, Avg loss: 0.349530 

Epoch 6
-------------------------------
loss: 0.149350  [   32/13903]
loss: 0.198043  [ 3232/13903]
loss: 0.225480  [ 6432/13903]
loss: 0.171192  [ 9632/13903]
loss: 0.286206  [12832/13903]
Test Error: 
 Accuracy: 88.0%, Avg loss: 0.313914 

Epoch 7
-------------------------------
loss: 0.144791  [   32/13903]
loss: 0.235686  [ 3232/13903]
loss: 0.225924  [ 6432/13903]
loss: 0.091289  [ 9632/13903]
loss: 0.301958  [12832/13903]
Test Error: 
 Accuracy: 83.6%, Avg loss: 0.489735 

Epoch 8
-------------------------------
loss: 0.131282  [   32/13903]
loss: 0.244590  [ 3232/13903]
loss: 0.215446  [ 6432/13903]
loss: 0.057560  [ 9632/13903]
loss: 0.306140  [12832/13903]
Test Error: 
 Accuracy: 86.7%, Avg loss: 0.378639 

Epoch 9
-------------------------------
loss: 0.128431  [   32/13903]
loss: 0.142801  [ 3232/13903]
loss: 0.176882  [ 6432/13903]
loss: 0.139240  [ 9632/13903]
loss: 0.213629  [12832/13903]
Test Error: 
 Accuracy: 80.3%, Avg loss: 0.626698 

Epoch 10
-------------------------------
loss: 0.137253  [   32/13903]
loss: 0.190767  [ 3232/13903]
loss: 0.284069  [ 6432/13903]
loss: 0.071140  [ 9632/13903]
loss: 0.183908  [12832/13903]
Test Error: 
 Accuracy: 81.0%, Avg loss: 0.590901 

Epoch 11
-------------------------------
loss: 0.059946  [   32/13903]
loss: 0.097621  [ 3232/13903]
loss: 0.171310  [ 6432/13903]
loss: 0.090907  [ 9632/13903]
loss: 0.262780  [12832/13903]
Test Error: 
 Accuracy: 84.5%, Avg loss: 0.474894 

Epoch 12
-------------------------------
loss: 0.200712  [   32/13903]
loss: 0.138416  [ 3232/13903]
loss: 0.095034  [ 6432/13903]
loss: 0.076186  [ 9632/13903]
loss: 0.058486  [12832/13903]
Test Error: 
 Accuracy: 86.4%, Avg loss: 0.428267 

Epoch 13
-------------------------------
loss: 0.058682  [   32/13903]
loss: 0.318619  [ 3232/13903]
loss: 0.145867  [ 6432/13903]
loss: 0.021354  [ 9632/13903]
loss: 0.046651  [12832/13903]
Test Error: 
 Accuracy: 87.5%, Avg loss: 0.452642 

Epoch 14
-------------------------------
loss: 0.031315  [   32/13903]
loss: 0.172306  [ 3232/13903]
loss: 0.092081  [ 6432/13903]
loss: 0.026590  [ 9632/13903]
loss: 0.083092  [12832/13903]
Test Error: 
 Accuracy: 86.9%, Avg loss: 0.450738 

Epoch 15
-------------------------------
loss: 0.075444  [   32/13903]
loss: 0.061268  [ 3232/13903]
loss: 0.056509  [ 6432/13903]
loss: 0.013620  [ 9632/13903]
loss: 0.047801  [12832/13903]
Test Error: 
 Accuracy: 84.9%, Avg loss: 0.600929 

Epoch 16
-------------------------------
loss: 0.059127  [   32/13903]
loss: 0.108923  [ 3232/13903]
loss: 0.025544  [ 6432/13903]
loss: 0.022174  [ 9632/13903]
loss: 0.027098  [12832/13903]
Test Error: 
 Accuracy: 89.2%, Avg loss: 0.451829 

Epoch 17
-------------------------------
loss: 0.002980  [   32/13903]
loss: 0.100589  [ 3232/13903]
loss: 0.021337  [ 6432/13903]
loss: 0.052322  [ 9632/13903]
loss: 0.023483  [12832/13903]
Test Error: 
 Accuracy: 87.2%, Avg loss: 0.647562 

Epoch 18
-------------------------------
loss: 0.051181  [   32/13903]
loss: 0.028157  [ 3232/13903]
loss: 0.058623  [ 6432/13903]
loss: 0.040707  [ 9632/13903]
loss: 0.033119  [12832/13903]
Test Error: 
 Accuracy: 90.7%, Avg loss: 0.459886 

Epoch 19
-------------------------------
loss: 0.002024  [   32/13903]
loss: 0.004977  [ 3232/13903]
loss: 0.001652  [ 6432/13903]
loss: 0.004471  [ 9632/13903]
loss: 0.001420  [12832/13903]
Test Error: 
 Accuracy: 90.6%, Avg loss: 0.529330 

Epoch 20
-------------------------------
loss: 0.000941  [   32/13903]
loss: 0.001437  [ 3232/13903]
loss: 0.000485  [ 6432/13903]
loss: 0.001236  [ 9632/13903]
loss: 0.000278  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 0.627210 

Epoch 21
-------------------------------
loss: 0.000425  [   32/13903]
loss: 0.000358  [ 3232/13903]
loss: 0.000131  [ 6432/13903]
loss: 0.000338  [ 9632/13903]
loss: 0.000075  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 0.737836 

Epoch 22
-------------------------------
loss: 0.000168  [   32/13903]
loss: 0.000114  [ 3232/13903]
loss: 0.000053  [ 6432/13903]
loss: 0.000133  [ 9632/13903]
loss: 0.000031  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 0.829103 

Epoch 23
-------------------------------
loss: 0.000075  [   32/13903]
loss: 0.000056  [ 3232/13903]
loss: 0.000031  [ 6432/13903]
loss: 0.000071  [ 9632/13903]
loss: 0.000017  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 0.898367 

Epoch 24
-------------------------------
loss: 0.000040  [   32/13903]
loss: 0.000035  [ 3232/13903]
loss: 0.000021  [ 6432/13903]
loss: 0.000045  [ 9632/13903]
loss: 0.000011  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 0.951916 

Epoch 25
-------------------------------
loss: 0.000024  [   32/13903]
loss: 0.000023  [ 3232/13903]
loss: 0.000015  [ 6432/13903]
loss: 0.000030  [ 9632/13903]
loss: 0.000008  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 0.996772 

Epoch 26
-------------------------------
loss: 0.000016  [   32/13903]
loss: 0.000016  [ 3232/13903]
loss: 0.000011  [ 6432/13903]
loss: 0.000021  [ 9632/13903]
loss: 0.000005  [12832/13903]
Test Error: 
 Accuracy: 90.4%, Avg loss: 1.036648 

Epoch 27
-------------------------------
loss: 0.000011  [   32/13903]
loss: 0.000011  [ 3232/13903]
loss: 0.000008  [ 6432/13903]
loss: 0.000015  [ 9632/13903]
loss: 0.000004  [12832/13903]
Test Error: 
 Accuracy: 90.4%, Avg loss: 1.073329 

Epoch 28
-------------------------------
loss: 0.000008  [   32/13903]
loss: 0.000008  [ 3232/13903]
loss: 0.000006  [ 6432/13903]
loss: 0.000011  [ 9632/13903]
loss: 0.000003  [12832/13903]
Test Error: 
 Accuracy: 90.4%, Avg loss: 1.106954 

Epoch 29
-------------------------------
loss: 0.000005  [   32/13903]
loss: 0.000006  [ 3232/13903]
loss: 0.000005  [ 6432/13903]
loss: 0.000010  [ 9632/13903]
loss: 0.000003  [12832/13903]
Test Error: 
 Accuracy: 90.4%, Avg loss: 1.110100 

Epoch 30
-------------------------------
loss: 0.000005  [   32/13903]
loss: 0.000006  [ 3232/13903]
loss: 0.000005  [ 6432/13903]
loss: 0.000009  [ 9632/13903]
loss: 0.000003  [12832/13903]
Test Error: 
 Accuracy: 90.4%, Avg loss: 1.114176 

Epoch 31
-------------------------------
loss: 0.000005  [   32/13903]
loss: 0.000006  [ 3232/13903]
loss: 0.000005  [ 6432/13903]
loss: 0.000009  [ 9632/13903]
loss: 0.000003  [12832/13903]
Test Error: 
 Accuracy: 90.4%, Avg loss: 1.118990 

Epoch 32
-------------------------------
loss: 0.000005  [   32/13903]
loss: 0.000005  [ 3232/13903]
loss: 0.000004  [ 6432/13903]
loss: 0.000009  [ 9632/13903]
loss: 0.000002  [12832/13903]
Test Error: 
 Accuracy: 90.4%, Avg loss: 1.124587 

Epoch 33
-------------------------------
loss: 0.000004  [   32/13903]
loss: 0.000005  [ 3232/13903]
loss: 0.000004  [ 6432/13903]
loss: 0.000008  [ 9632/13903]
loss: 0.000002  [12832/13903]
Test Error: 
 Accuracy: 90.4%, Avg loss: 1.131066 

Epoch 34
-------------------------------
loss: 0.000004  [   32/13903]
loss: 0.000005  [ 3232/13903]
loss: 0.000004  [ 6432/13903]
loss: 0.000008  [ 9632/13903]
loss: 0.000002  [12832/13903]
Test Error: 
 Accuracy: 90.4%, Avg loss: 1.138468 

Epoch 35
-------------------------------
loss: 0.000004  [   32/13903]
loss: 0.000004  [ 3232/13903]
loss: 0.000004  [ 6432/13903]
loss: 0.000007  [ 9632/13903]
loss: 0.000002  [12832/13903]
Test Error: 
 Accuracy: 90.4%, Avg loss: 1.146812 

Epoch 36
-------------------------------
loss: 0.000003  [   32/13903]
loss: 0.000004  [ 3232/13903]
loss: 0.000003  [ 6432/13903]
loss: 0.000006  [ 9632/13903]
loss: 0.000002  [12832/13903]
Test Error: 
 Accuracy: 90.4%, Avg loss: 1.156104 

Epoch 37
-------------------------------
loss: 0.000003  [   32/13903]
loss: 0.000004  [ 3232/13903]
loss: 0.000003  [ 6432/13903]
loss: 0.000006  [ 9632/13903]
loss: 0.000002  [12832/13903]
Test Error: 
 Accuracy: 90.4%, Avg loss: 1.166293 

Epoch 38
-------------------------------
loss: 0.000003  [   32/13903]
loss: 0.000003  [ 3232/13903]
loss: 0.000003  [ 6432/13903]
loss: 0.000005  [ 9632/13903]
loss: 0.000002  [12832/13903]
Test Error: 
 Accuracy: 90.4%, Avg loss: 1.177345 

Epoch 39
-------------------------------
loss: 0.000002  [   32/13903]
loss: 0.000003  [ 3232/13903]
loss: 0.000002  [ 6432/13903]
loss: 0.000005  [ 9632/13903]
loss: 0.000001  [12832/13903]
Test Error: 
 Accuracy: 90.4%, Avg loss: 1.189175 

Epoch 40
-------------------------------
loss: 0.000002  [   32/13903]
loss: 0.000003  [ 3232/13903]
loss: 0.000002  [ 6432/13903]
loss: 0.000005  [ 9632/13903]
loss: 0.000001  [12832/13903]
Test Error: 
 Accuracy: 90.4%, Avg loss: 1.190399 

Epoch 41
-------------------------------
loss: 0.000002  [   32/13903]
loss: 0.000003  [ 3232/13903]
loss: 0.000002  [ 6432/13903]
loss: 0.000004  [ 9632/13903]
loss: 0.000001  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 1.191902 

Epoch 42
-------------------------------
loss: 0.000002  [   32/13903]
loss: 0.000003  [ 3232/13903]
loss: 0.000002  [ 6432/13903]
loss: 0.000004  [ 9632/13903]
loss: 0.000001  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 1.193632 

Epoch 43
-------------------------------
loss: 0.000002  [   32/13903]
loss: 0.000003  [ 3232/13903]
loss: 0.000002  [ 6432/13903]
loss: 0.000004  [ 9632/13903]
loss: 0.000001  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 1.195601 

Epoch 44
-------------------------------
loss: 0.000002  [   32/13903]
loss: 0.000003  [ 3232/13903]
loss: 0.000002  [ 6432/13903]
loss: 0.000004  [ 9632/13903]
loss: 0.000001  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 1.197819 

Epoch 45
-------------------------------
loss: 0.000002  [   32/13903]
loss: 0.000003  [ 3232/13903]
loss: 0.000002  [ 6432/13903]
loss: 0.000004  [ 9632/13903]
loss: 0.000001  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 1.200291 

Epoch 46
-------------------------------
loss: 0.000002  [   32/13903]
loss: 0.000002  [ 3232/13903]
loss: 0.000002  [ 6432/13903]
loss: 0.000004  [ 9632/13903]
loss: 0.000001  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 1.203025 

Epoch 47
-------------------------------
loss: 0.000002  [   32/13903]
loss: 0.000002  [ 3232/13903]
loss: 0.000002  [ 6432/13903]
loss: 0.000004  [ 9632/13903]
loss: 0.000001  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 1.206020 

Epoch 48
-------------------------------
loss: 0.000002  [   32/13903]
loss: 0.000002  [ 3232/13903]
loss: 0.000002  [ 6432/13903]
loss: 0.000004  [ 9632/13903]
loss: 0.000001  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 1.209277 

Epoch 49
-------------------------------
loss: 0.000002  [   32/13903]
loss: 0.000002  [ 3232/13903]
loss: 0.000002  [ 6432/13903]
loss: 0.000004  [ 9632/13903]
loss: 0.000001  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 1.212795 

Epoch 50
-------------------------------
loss: 0.000001  [   32/13903]
loss: 0.000002  [ 3232/13903]
loss: 0.000002  [ 6432/13903]
loss: 0.000003  [ 9632/13903]
loss: 0.000001  [12832/13903]
Test Error: 
 Accuracy: 90.5%, Avg loss: 1.216567 

Done!

